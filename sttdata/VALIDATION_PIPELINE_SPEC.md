# TTS Validation Pipeline - Build Spec

## Goal
Build an automated TTS quality validation system that accurately identifies **Kokoro TTS pronunciation failures** by distinguishing them from **STT transcription errors**. This is the core quality gate for a TTS training data pipeline.

## Architecture: Tiered Multi-Engine STT Validation

Two STT engines with different speed/accuracy tradeoffs, used in sequence:

| Engine | Speed | WER | Role |
|--------|-------|-----|------|
| **NVIDIA Parakeet** (already running) | ~500x realtime | ~6% | Fast bulk scanner — first pass on all audio |
| **Qwen2-Audio 0.5B** (already available) | Slower | ~2% | Precision validator — second pass on flagged words only |

### Why Two Engines
A single STT engine can't distinguish its own transcription errors from actual TTS pronunciation failures. If Parakeet transcribes "advice" as "addvice", we don't know if Kokoro mispronounced it or Parakeet misheard it. Running a second, more accurate engine on disputed words resolves this.

## Pipeline Flow

### Input
Each validation job consists of:
- **Audio file** (.wav, 24kHz, 16-bit PCM, mono) — generated by Kokoro TTS
- **Ground truth text** (.txt) — the original text that was fed to Kokoro

Audio files are typically ~30 seconds each (training chunk size), but the system should handle up to 20+ minutes.

### Step 1: Parakeet Bulk Scan (Fast Pass)
Run the full audio through Parakeet. Parakeet returns:
- Full transcription text
- **Word-level timestamps** (start_time, end_time per word)
- **Word-level confidence scores** (5 significant figures, 0.0 to 1.0)

Align Parakeet's transcription against ground truth word-by-word. For each word, determine:
- **PASS**: Parakeet transcription matches ground truth (case-insensitive, stripped punctuation)
- **FLAG**: Parakeet transcription does NOT match ground truth

Words with confidence ≥ 0.99 AND matching ground truth can be marked PASS with high certainty.

**Output of Step 1**: List of flagged words with their:
- Ground truth word
- Parakeet transcription
- Parakeet confidence score
- Audio timestamp (start_time, end_time) from Parakeet's word-level data
- Word index in the sequence

### Step 2: Qwen Precision Validation (Flagged Words Only)
For each flagged word from Step 1:
- Extract the audio segment using the timestamp (±0.25s padding for context)
- OR: group nearby flagged words into segments to reduce API calls
- Run the audio segment through Qwen2-Audio
- Compare Qwen's transcription of that segment against ground truth

### Step 3: Voting / Final Verdict

For each flagged word, apply this decision matrix:

| Parakeet vs Ground Truth | Qwen vs Ground Truth | Verdict | Action |
|--------------------------|----------------------|---------|--------|
| ✗ mismatch | ✓ match | **STT Error** (Parakeet wrong) | Ignore — Kokoro pronounced it fine |
| ✗ mismatch | ✗ mismatch | **TTS Failure** (Kokoro wrong) | Flag as real pronunciation error |
| ✗ mismatch | ✗ different mismatch | **Ambiguous** | Flag for manual review or use confidence scores to break tie |

### Step 4: Output Report
For each audio file processed, output a JSON report:

```json
{
  "audio_file": "chunk_0042.wav",
  "ground_truth_file": "chunk_0042.txt",
  "audio_duration_s": 22.5,
  "total_words": 75,
  "processing_time_ms": {
    "parakeet_ms": 45,
    "qwen_ms": 320,
    "total_ms": 365
  },
  "summary": {
    "pass": 70,
    "stt_error": 3,
    "tts_failure": 1,
    "ambiguous": 1,
    "pass_rate": 0.9333,
    "tts_failure_rate": 0.0133
  },
  "parakeet_stats": {
    "mean_confidence": 0.985,
    "median_confidence": 0.999,
    "min_confidence": 0.684,
    "words_below_90": 3,
    "words_below_95": 5
  },
  "failures": [
    {
      "word_index": 12,
      "ground_truth": "illustration",
      "parakeet_transcription": "illustr",
      "parakeet_confidence": 0.72341,
      "qwen_transcription": "illustra",
      "verdict": "tts_failure",
      "timestamp": {"start": 4.21, "end": 4.58},
      "context": "...the illustration of Alice's..."
    }
  ],
  "stt_errors": [
    {
      "word_index": 8,
      "ground_truth": "rabbit-hole",
      "parakeet_transcription": "rabbitole",
      "parakeet_confidence": 0.91234,
      "qwen_transcription": "rabbit hole",
      "verdict": "stt_error",
      "note": "Parakeet failed on hyphenated compound; Qwen got it right"
    }
  ]
}
```

## Batch Processing Mode

The system should support batch processing a directory of paired .wav + .txt files:

```bash
python validate_tts.py --input-dir /path/to/chunks/ --output-dir /path/to/reports/
```

Expected input directory structure:
```
chunks/
  chunk_0000.wav
  chunk_0000.txt
  chunk_0001.wav
  chunk_0001.txt
  ...
```

Output: One JSON report per file, plus an aggregate summary:

```json
{
  "total_files": 378,
  "total_words": 26525,
  "total_audio_duration_s": 8146,
  "total_processing_time_s": 18.5,
  "aggregate_pass_rate": 0.972,
  "aggregate_tts_failure_rate": 0.008,
  "aggregate_stt_error_rate": 0.020,
  "top_failure_words": [
    {"word": "illustration", "failures": 3, "contexts": ["...", "..."]},
    {"word": "3.0", "failures": 2, "contexts": ["..."]}
  ],
  "failure_categories": {
    "truncated_words": 12,
    "number_pronunciation": 8,
    "hyphenated_compounds": 5,
    "proper_nouns": 3,
    "other": 4
  }
}
```

## Word Alignment Strategy

Aligning STT output to ground truth word-by-word is non-trivial because:
- STT may merge words ("rabbit hole" → "rabbithole")
- STT may split words ("three-legged" → "three legged")
- STT may skip or insert words
- Punctuation and casing differ

**Recommended approach**: Use a sequence alignment algorithm (Levenshtein / edit distance at the word level). Libraries like `jiwer` (Python) handle this well — it computes WER and provides word-level alignments (insertions, deletions, substitutions).

```python
# jiwer gives you exactly this
from jiwer import process_words
result = process_words(reference=ground_truth, hypothesis=transcription)
# result.alignments gives word-by-word alignment with types: hit, substitution, insertion, deletion
```

**Normalization before comparison:**
- Lowercase everything
- Strip punctuation (periods, commas, quotes, dashes, etc.)
- Normalize numbers: "3.0" and "three point zero" should be compared intelligently
- Handle contractions: "don't" = "dont" = "do n't"
- Handle possessives: "Alice's" = "alices"

## Existing Infrastructure

### Parakeet
- Already running and tested
- Returns word-level timestamps and confidence scores
- Tested at 507x realtime on 20-min audio (2.4 seconds processing)
- 24kHz mono WAV input

### Qwen2-Audio
- Available to Eugene (already has access)
- ~2% WER — significantly more accurate than Parakeet
- Slower but only needs to process ~6% of words (the flagged ones)

### Kokoro TTS
- Running on localhost:8880
- OpenAI-compatible API
- Can generate test audio: `POST /v1/audio/speech` with `{"input": "text", "voice": "af_heart", "speed": 1.0, "response_format": "wav"}`

### Test Data Available
- `/home/echo/projects/kokoro/sttdata/dev_clean/LibriTTS_R/dev-clean/SAMPLE TEST/book_chunks/` — 378 paired .wav + .txt files (Alice in Wonderland, ~2.25 hours total)
- Each chunk is ~20-30 seconds, ~75 words
- Generated by Kokoro at 24kHz, 16-bit PCM, mono, voice=af_heart, speed=1.0

## Performance Targets

| Metric | Target |
|--------|--------|
| Bulk scan speed | ≥100x realtime (Parakeet handles this) |
| End-to-end per chunk | <1 second for 30s audio |
| Batch 378 files | <2 minutes total |
| False positive rate | <1% (don't flag Kokoro for STT errors) |

## Future Extensions (Not Required Now)

These are planned but don't build them yet:
- **Failure categorization**: Auto-classify failures (numbers, compounds, truncation, etc.)
- **Retraining trigger**: When failure rate exceeds threshold, queue for targeted training
- **ElevenLabs fallback**: For confirmed Kokoro failures, generate correct audio via ElevenLabs for training data
- **Multi-voice validation**: Run same text through multiple Kokoro voices, compare failure patterns
